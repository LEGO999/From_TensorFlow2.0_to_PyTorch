{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH / 'mnist'\n",
    "\n",
    "# Make parent directory if not existed, ignore the error if directory existed\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# This is pkl not pk1\n",
    "URL = 'http://deeplearning.net/data/mnist/'\n",
    "FILENAME = 'mnist.pkl.gz'\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "#     use requests to get the file, access the response body through content through bytes/non-text\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "#     writing + binary\n",
    "    (PATH / FILENAME).open('wb').write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# as_posix() windows_path --> posix_path(linux)\n",
    "with gzip.open((PATH / FILENAME).as_posix(), 'rb') as f:\n",
    "#     pickle is a Python module to seralize and deseralize data\n",
    "    ((x_train, y_train), (x_val, y_val), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(x_train[0].reshape((28,28)), cmap='gray')\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "torch.Size([50000])\n",
      "tensor(0.) tensor(0.9961)\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# torch.tensor 可以construct tensor，这里使用的是复制numpy array的方式\n",
    "# 如果不想复制numpy array的话，可以用torch.as_tensor()\n",
    "x_train, y_train, x_val, y_val = map(torch.tensor, (x_train, y_train, x_val, y_val))\n",
    "\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_train.min(), x_train.max())\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Xavier innitialisation: weights matrix from [0, 1]divided by sqrt(n) \n",
    "# n: number of neurons in the corresponding layer\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "# '_' trailing: performed in-place\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "#     例如output[32, 10] --> [32, 1]-->[32,]\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "#     这里也可以用Tensor.mm()\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.1787, -1.8506, -2.0425, -2.3958, -2.2967, -2.6759, -2.3255, -2.8313,\n",
      "        -2.7089, -2.1529], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# batch_size\n",
    "bs = 64\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非常小心的一点是，这是说是NLL negative log likelihood loss，但是其实只是mean nagative likelihood loss 而已\n",
    "# 网络的输出已经经过log了\n",
    "# 这里使用了range(target.shape[0])， 实际输出range(0, 64)，意思是在batch_size这个dimension 上选0-63，\n",
    "# 而对应每个sample 取target 那个的标签。这个功能也可以用torch.gather()实现\n",
    "def nll(input, target):\n",
    "    batch_range = range(target.shape[0])\n",
    "    return -input[batch_range, target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./fig/Tensor_index.png' width=700 align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4295, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return(preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0156)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4295337200164795\n",
      "1.9695050716400146\n",
      "1.9009286165237427\n",
      "1.6275380849838257\n",
      "1.355352520942688\n",
      "1.0061415433883667\n",
      "0.922238290309906\n",
      "1.0488682985305786\n",
      "0.9463463425636292\n",
      "1.1573047637939453\n",
      "0.9329898953437805\n",
      "0.9808358550071716\n",
      "0.8489398956298828\n",
      "0.9802277684211731\n",
      "1.0543029308319092\n",
      "0.7514306902885437\n",
      "1.004091739654541\n",
      "0.9661435484886169\n",
      "0.5454010367393494\n",
      "0.6689472198486328\n",
      "0.7393567562103271\n",
      "1.024264931678772\n",
      "1.095572590827942\n",
      "0.9292601943016052\n",
      "0.5112167596817017\n",
      "0.5293198823928833\n",
      "0.41545557975769043\n",
      "0.5275112390518188\n",
      "0.5792365074157715\n",
      "0.47159576416015625\n",
      "0.49069684743881226\n",
      "0.5089037418365479\n",
      "0.4620976746082306\n",
      "0.3331240713596344\n",
      "0.4982525408267975\n",
      "0.4495408535003662\n",
      "0.3910275101661682\n",
      "0.6135691404342651\n",
      "0.4904988706111908\n",
      "0.34612125158309937\n",
      "0.4225466251373291\n",
      "0.5994874238967896\n",
      "0.6270489692687988\n",
      "0.5292664170265198\n",
      "0.3434979021549225\n",
      "0.4801288843154907\n",
      "0.4175183176994324\n",
      "0.6124203205108643\n",
      "0.4209957420825958\n",
      "0.3572678565979004\n",
      "0.42078161239624023\n",
      "0.46841394901275635\n",
      "0.4061775803565979\n",
      "0.454256147146225\n",
      "0.4219716787338257\n",
      "0.5062651634216309\n",
      "0.30098211765289307\n",
      "0.5320677757263184\n",
      "0.5605601668357849\n",
      "0.4703863859176636\n",
      "0.3091707229614258\n",
      "0.3684659004211426\n",
      "0.47745195031166077\n",
      "0.5665251612663269\n",
      "0.5010058879852295\n",
      "0.4372052252292633\n",
      "0.45766302943229675\n",
      "0.4442102909088135\n",
      "0.3997940421104431\n",
      "0.3846060335636139\n",
      "0.36771291494369507\n",
      "0.22250419855117798\n",
      "0.4671856760978699\n",
      "0.4440324604511261\n",
      "0.5061481595039368\n",
      "0.5013185143470764\n",
      "0.2931040823459625\n",
      "0.5550341010093689\n",
      "0.3856581449508667\n",
      "0.4358711540699005\n",
      "0.7619116902351379\n",
      "0.27987584471702576\n",
      "0.4705393612384796\n",
      "0.41176509857177734\n",
      "0.3249821662902832\n",
      "0.2479020059108734\n",
      "0.4910356104373932\n",
      "0.38573622703552246\n",
      "0.35635578632354736\n",
      "0.4198935925960541\n",
      "0.30005118250846863\n",
      "0.46941936016082764\n",
      "0.36322021484375\n",
      "0.3348767161369324\n",
      "0.23726896941661835\n",
      "0.3841075003147125\n",
      "0.32402199506759644\n",
      "0.34758055210113525\n",
      "0.28007060289382935\n",
      "0.2756040096282959\n",
      "0.3125341534614563\n",
      "0.5101729035377502\n",
      "0.25540030002593994\n",
      "0.3197799623012543\n",
      "0.3304253816604614\n",
      "0.2754881680011749\n",
      "0.5107396841049194\n",
      "0.47023361921310425\n",
      "0.5901243686676025\n",
      "0.6863619089126587\n",
      "0.29976385831832886\n",
      "0.44765782356262207\n",
      "0.5023488998413086\n",
      "0.6737580895423889\n",
      "0.5814930200576782\n",
      "0.3096272647380829\n",
      "0.3311998248100281\n",
      "0.4284420907497406\n",
      "0.24914243817329407\n",
      "0.4952596127986908\n",
      "0.35751476883888245\n",
      "0.41128379106521606\n",
      "0.5345820188522339\n",
      "0.5240692496299744\n",
      "0.37787747383117676\n",
      "0.33378836512565613\n",
      "0.5288155674934387\n",
      "0.2396588921546936\n",
      "0.6706119179725647\n",
      "0.4583624601364136\n",
      "0.26117420196533203\n",
      "0.45944058895111084\n",
      "0.5147112607955933\n",
      "0.3015652894973755\n",
      "0.31315621733665466\n",
      "0.6916569471359253\n",
      "0.8279304504394531\n",
      "0.558938205242157\n",
      "0.7340462803840637\n",
      "0.5423439741134644\n",
      "0.3227565884590149\n",
      "0.2029874622821808\n",
      "0.48194435238838196\n",
      "0.25192058086395264\n",
      "0.500204861164093\n",
      "0.4109193980693817\n",
      "0.4777858257293701\n",
      "0.6081297993659973\n",
      "0.3012295961380005\n",
      "0.3873435854911804\n",
      "0.2667739987373352\n",
      "0.20263001322746277\n",
      "0.5684205293655396\n",
      "0.21871814131736755\n",
      "0.2578742206096649\n",
      "0.34241053462028503\n",
      "0.40330374240875244\n",
      "0.2544436752796173\n",
      "0.2554604411125183\n",
      "0.45098406076431274\n",
      "0.3677051067352295\n",
      "0.2220258116722107\n",
      "0.23633769154548645\n",
      "0.3060126006603241\n",
      "0.15110476315021515\n",
      "0.1892596185207367\n",
      "0.3110315203666687\n",
      "0.43818342685699463\n",
      "0.37262099981307983\n",
      "0.28825661540031433\n",
      "0.2812127470970154\n",
      "0.30647599697113037\n",
      "0.2567770481109619\n",
      "0.2977669835090637\n",
      "0.2027471959590912\n",
      "0.4642941951751709\n",
      "0.19634471833705902\n",
      "0.28718242049217224\n",
      "0.25743669271469116\n",
      "0.3180104196071625\n",
      "0.6291961073875427\n",
      "0.5191112160682678\n",
      "0.5804517269134521\n",
      "0.4492054879665375\n",
      "0.278934121131897\n",
      "0.40281111001968384\n",
      "0.27874037623405457\n",
      "0.2769954204559326\n",
      "0.3826884329319\n",
      "0.273762583732605\n",
      "0.4715654253959656\n",
      "0.39538294076919556\n",
      "0.3742380440235138\n",
      "0.44029587507247925\n",
      "0.40686163306236267\n",
      "0.4641944169998169\n",
      "0.6675227880477905\n",
      "0.7616596221923828\n",
      "0.5857641100883484\n",
      "0.4569166898727417\n",
      "0.3032124638557434\n",
      "0.30119630694389343\n",
      "0.5856547355651855\n",
      "0.640598475933075\n",
      "0.5777296423912048\n",
      "0.49397289752960205\n",
      "0.3373548984527588\n",
      "0.34228044748306274\n",
      "0.382679283618927\n",
      "0.33167847990989685\n",
      "0.23956340551376343\n",
      "0.43618524074554443\n",
      "0.16554024815559387\n",
      "0.5437239408493042\n",
      "0.5199376344680786\n",
      "0.2656857669353485\n",
      "0.4035549759864807\n",
      "0.4613635241985321\n",
      "0.6522918939590454\n",
      "0.522162675857544\n",
      "0.4557638168334961\n",
      "0.44184526801109314\n",
      "0.4108844995498657\n",
      "0.5443047285079956\n",
      "0.7389011979103088\n",
      "0.38344043493270874\n",
      "0.3681083917617798\n",
      "0.4784793555736542\n",
      "0.5004839897155762\n",
      "0.5103817582130432\n",
      "0.5852829217910767\n",
      "0.6904147267341614\n",
      "0.3385963439941406\n",
      "0.2559327483177185\n",
      "0.24574609100818634\n",
      "0.2545662522315979\n",
      "0.5831394195556641\n",
      "0.3714650273323059\n",
      "0.3525281548500061\n",
      "0.2632638216018677\n",
      "0.2608896791934967\n",
      "0.2948499917984009\n",
      "0.4080975651741028\n",
      "0.32925015687942505\n",
      "0.2631349563598633\n",
      "0.43924567103385925\n",
      "0.43875041604042053\n",
      "0.37702661752700806\n",
      "0.36863240599632263\n",
      "0.6759153604507446\n",
      "0.5539788603782654\n",
      "0.3129054009914398\n",
      "0.20516709983348846\n",
      "0.33965298533439636\n",
      "0.3221827447414398\n",
      "0.30954256653785706\n",
      "0.24688009917736053\n",
      "0.3522070348262787\n",
      "0.4668007493019104\n",
      "0.3402288556098938\n",
      "0.4488202631473541\n",
      "0.34760963916778564\n",
      "0.43713366985321045\n",
      "0.32254862785339355\n",
      "0.34230005741119385\n",
      "0.44293978810310364\n",
      "0.3880729079246521\n",
      "0.35435283184051514\n",
      "0.4501039981842041\n",
      "0.44294610619544983\n",
      "0.21083807945251465\n",
      "0.36395010352134705\n",
      "0.28595495223999023\n",
      "0.48072007298469543\n",
      "0.6654396057128906\n",
      "0.31058478355407715\n",
      "0.4781278669834137\n",
      "0.3512578010559082\n",
      "0.5796197056770325\n",
      "0.3448394536972046\n",
      "0.2686092257499695\n",
      "0.3117487132549286\n",
      "0.3121170699596405\n",
      "0.2103663831949234\n",
      "0.19791436195373535\n",
      "0.2608162462711334\n",
      "0.2841200828552246\n",
      "0.33979904651641846\n",
      "0.3506963551044464\n",
      "0.2428383231163025\n",
      "0.45280513167381287\n",
      "0.26651647686958313\n",
      "0.37352442741394043\n",
      "0.17998665571212769\n",
      "0.35551679134368896\n",
      "0.21689920127391815\n",
      "0.23323075473308563\n",
      "0.361135870218277\n",
      "0.3643248379230499\n",
      "0.29682907462120056\n",
      "0.3935190439224243\n",
      "0.3514813780784607\n",
      "0.35680097341537476\n",
      "0.37134698033332825\n",
      "0.3419383466243744\n",
      "0.39805868268013\n",
      "0.26673269271850586\n",
      "0.16247780621051788\n",
      "0.10260210186243057\n",
      "0.44537755846977234\n",
      "0.23411455750465393\n",
      "0.3958182632923126\n",
      "0.42727187275886536\n",
      "0.4023618996143341\n",
      "0.2558373212814331\n",
      "0.589463472366333\n",
      "0.26325589418411255\n",
      "0.3217654824256897\n",
      "0.19256332516670227\n",
      "0.23917114734649658\n",
      "0.22670984268188477\n",
      "0.4830579161643982\n",
      "0.28591275215148926\n",
      "0.49616092443466187\n",
      "0.3667820692062378\n",
      "0.4507339298725128\n",
      "0.5810937881469727\n",
      "0.6947115659713745\n",
      "0.43172502517700195\n",
      "0.3801027238368988\n",
      "0.30878040194511414\n",
      "0.31453919410705566\n",
      "0.20941191911697388\n",
      "0.304178923368454\n",
      "0.38746219873428345\n",
      "0.13999979197978973\n",
      "0.2551310062408447\n",
      "0.5535402894020081\n",
      "0.2691897451877594\n",
      "0.2837839424610138\n",
      "0.0993390753865242\n",
      "0.14840427041053772\n",
      "0.30779212713241577\n",
      "0.3499602675437927\n",
      "0.1716877818107605\n",
      "0.4586280286312103\n",
      "0.460732102394104\n",
      "0.4746478497982025\n",
      "0.33593693375587463\n",
      "0.11124128103256226\n",
      "0.30720096826553345\n",
      "0.6616132855415344\n",
      "0.7959312796592712\n",
      "0.5185425281524658\n",
      "0.32224637269973755\n",
      "0.34426096081733704\n",
      "0.31049787998199463\n",
      "0.23207911849021912\n",
      "0.12616881728172302\n",
      "0.26979130506515503\n",
      "0.44752663373947144\n",
      "0.36513909697532654\n",
      "0.3513977825641632\n",
      "0.18143445253372192\n",
      "0.26665687561035156\n",
      "0.26490938663482666\n",
      "0.33652156591415405\n",
      "0.2994464337825775\n",
      "0.25190532207489014\n",
      "0.29556533694267273\n",
      "0.5860227942466736\n",
      "0.24123349785804749\n",
      "0.4251249432563782\n",
      "0.3455354571342468\n",
      "0.46583956480026245\n",
      "0.3917797803878784\n",
      "0.2539493143558502\n",
      "0.22889278829097748\n",
      "0.4078163206577301\n",
      "0.46780481934547424\n",
      "0.24520662426948547\n",
      "0.28372621536254883\n",
      "0.37984010577201843\n",
      "0.457986980676651\n",
      "0.37423405051231384\n",
      "0.4124085307121277\n",
      "0.3640194535255432\n",
      "0.526091992855072\n",
      "0.30323827266693115\n",
      "0.3947194516658783\n",
      "0.3597812056541443\n",
      "0.1514325886964798\n",
      "0.3416016399860382\n",
      "0.3113875389099121\n",
      "0.291439026594162\n",
      "0.5082662105560303\n",
      "0.1322106570005417\n",
      "0.12104541063308716\n",
      "0.24057349562644958\n",
      "0.3533724248409271\n",
      "0.2373848855495453\n",
      "0.22907905280590057\n",
      "0.26250430941581726\n",
      "0.4276886582374573\n",
      "0.20398089289665222\n",
      "0.2718833088874817\n",
      "0.2435505986213684\n",
      "0.2428773194551468\n",
      "0.21650545299053192\n",
      "0.2171947956085205\n",
      "0.355033814907074\n",
      "0.3072403073310852\n",
      "0.5986250042915344\n",
      "0.4264536499977112\n",
      "0.3590010404586792\n",
      "0.3922848701477051\n",
      "0.6148412227630615\n",
      "0.6221562027931213\n",
      "0.38457247614860535\n",
      "0.4110172390937805\n",
      "0.5080955028533936\n",
      "0.145133838057518\n",
      "0.19473767280578613\n",
      "0.2657051086425781\n",
      "0.6904742121696472\n",
      "0.3672332763671875\n",
      "0.1910463571548462\n",
      "0.3120971620082855\n",
      "0.3424927592277527\n",
      "0.42831048369407654\n",
      "0.22610163688659668\n",
      "0.3742196559906006\n",
      "0.3027704358100891\n",
      "0.2520114779472351\n",
      "0.39200252294540405\n",
      "0.23652207851409912\n",
      "0.19970573484897614\n",
      "0.22488518059253693\n",
      "0.1868637502193451\n",
      "0.33058199286460876\n",
      "0.31618523597717285\n",
      "0.24928034842014313\n",
      "0.15369673073291779\n",
      "0.7028229832649231\n",
      "0.23861344158649445\n",
      "0.34801554679870605\n",
      "0.3180423080921173\n",
      "0.9229645729064941\n",
      "0.30845674872398376\n",
      "0.2713695764541626\n",
      "0.2087780237197876\n",
      "0.15982681512832642\n",
      "0.322107195854187\n",
      "0.16010792553424835\n",
      "0.43709635734558105\n",
      "0.4973964989185333\n",
      "0.4273238182067871\n",
      "0.34476137161254883\n",
      "0.31124645471572876\n",
      "0.3429757058620453\n",
      "0.23703701794147491\n",
      "0.26937827467918396\n",
      "0.2828975319862366\n",
      "0.33910563588142395\n",
      "0.5054342746734619\n",
      "0.25214478373527527\n",
      "0.5892655253410339\n",
      "0.51370769739151\n",
      "0.4850640892982483\n",
      "0.38261789083480835\n",
      "0.4058464765548706\n",
      "0.49535897374153137\n",
      "0.41462284326553345\n",
      "0.23802970349788666\n",
      "0.1870468258857727\n",
      "0.31873366236686707\n",
      "0.4074136018753052\n",
      "0.3032148778438568\n",
      "0.4862814247608185\n",
      "0.392900288105011\n",
      "0.3766035735607147\n",
      "0.3342844545841217\n",
      "0.4836124777793884\n",
      "0.27218395471572876\n",
      "0.42426156997680664\n",
      "0.32673633098602295\n",
      "0.5147374272346497\n",
      "0.5755705833435059\n",
      "0.40803271532058716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774145245552063\n",
      "0.5771186947822571\n",
      "0.40142613649368286\n",
      "0.1701907068490982\n",
      "0.30918264389038086\n",
      "0.49469906091690063\n",
      "0.6295095682144165\n",
      "0.40859436988830566\n",
      "0.29049602150917053\n",
      "0.2993665039539337\n",
      "0.3673185408115387\n",
      "0.3765139877796173\n",
      "0.37049537897109985\n",
      "0.4729433059692383\n",
      "0.3676651418209076\n",
      "0.4948062002658844\n",
      "0.480194091796875\n",
      "0.4796765446662903\n",
      "0.48111289739608765\n",
      "0.2642470598220825\n",
      "0.12928080558776855\n",
      "0.4478495121002197\n",
      "0.30694836378097534\n",
      "0.5541710257530212\n",
      "0.27768003940582275\n",
      "0.36309903860092163\n",
      "0.2535833716392517\n",
      "0.2835923135280609\n",
      "0.23566433787345886\n",
      "0.2300799936056137\n",
      "0.32570573687553406\n",
      "0.38057783246040344\n",
      "0.4244438409805298\n",
      "0.3660382330417633\n",
      "0.3018668293952942\n",
      "0.2872979938983917\n",
      "0.1764853149652481\n",
      "0.1824893355369568\n",
      "0.39774033427238464\n",
      "0.15273337066173553\n",
      "0.1006225198507309\n",
      "0.16070368885993958\n",
      "0.18950322270393372\n",
      "0.28216421604156494\n",
      "0.19816112518310547\n",
      "0.23087626695632935\n",
      "0.20542722940444946\n",
      "0.19989335536956787\n",
      "0.32026106119155884\n",
      "0.47354668378829956\n",
      "0.537994384765625\n",
      "0.3364085257053375\n",
      "0.47873181104660034\n",
      "0.4241286516189575\n",
      "0.372018039226532\n",
      "0.5283380150794983\n",
      "0.3322649598121643\n",
      "0.178333580493927\n",
      "0.2918195128440857\n",
      "0.18886616826057434\n",
      "0.28814536333084106\n",
      "0.36330556869506836\n",
      "0.21557724475860596\n",
      "0.2081826627254486\n",
      "0.31392210721969604\n",
      "0.47330427169799805\n",
      "0.18431644141674042\n",
      "0.33627063035964966\n",
      "0.2713775634765625\n",
      "0.19095571339130402\n",
      "0.1646009087562561\n",
      "0.2713722586631775\n",
      "0.31173229217529297\n",
      "0.391996294260025\n",
      "0.37784385681152344\n",
      "0.31246471405029297\n",
      "0.27909860014915466\n",
      "0.2382195144891739\n",
      "0.12164958566427231\n",
      "0.30617010593414307\n",
      "0.3883291482925415\n",
      "0.19594615697860718\n",
      "0.2532716989517212\n",
      "0.15166933834552765\n",
      "0.18818959593772888\n",
      "0.32624635100364685\n",
      "0.3022560775279999\n",
      "0.3069438338279724\n",
      "0.18205109238624573\n",
      "0.30251583456993103\n",
      "0.45090967416763306\n",
      "0.4348990321159363\n",
      "0.27200403809547424\n",
      "0.6435838937759399\n",
      "0.7132241725921631\n",
      "0.9513505697250366\n",
      "0.5394788980484009\n",
      "0.5214962959289551\n",
      "0.26246318221092224\n",
      "0.2848018407821655\n",
      "0.4918562173843384\n",
      "0.29396066069602966\n",
      "0.36108601093292236\n",
      "0.28464794158935547\n",
      "0.4144331216812134\n",
      "0.3125593066215515\n",
      "0.2266622930765152\n",
      "0.19090960919857025\n",
      "0.4143775701522827\n",
      "0.318882018327713\n",
      "0.34383875131607056\n",
      "0.2634162902832031\n",
      "0.366263210773468\n",
      "0.43276098370552063\n",
      "0.4733176529407501\n",
      "0.35527142882347107\n",
      "0.295849084854126\n",
      "0.1837267428636551\n",
      "0.18233869969844818\n",
      "0.19282057881355286\n",
      "0.2468542903661728\n",
      "0.1620163768529892\n",
      "0.2997989058494568\n",
      "0.24392089247703552\n",
      "0.2870361804962158\n",
      "0.8216602802276611\n",
      "0.40563899278640747\n",
      "0.5563652515411377\n",
      "0.27075397968292236\n",
      "0.2600990831851959\n",
      "0.3435037434101105\n",
      "0.28708457946777344\n",
      "0.3735235631465912\n",
      "0.25766894221305847\n",
      "0.32607316970825195\n",
      "0.46596118807792664\n",
      "0.24153004586696625\n",
      "0.3479319214820862\n",
      "0.2964811325073242\n",
      "0.21541935205459595\n",
      "0.29352015256881714\n",
      "0.29181379079818726\n",
      "0.23676738142967224\n",
      "0.4071953594684601\n",
      "0.33949315547943115\n",
      "0.3059034049510956\n",
      "0.3049919605255127\n",
      "0.3735637962818146\n",
      "0.20985127985477448\n",
      "0.18345573544502258\n",
      "0.2195967435836792\n",
      "0.4812942147254944\n",
      "0.25310254096984863\n",
      "0.2991997301578522\n",
      "0.22381678223609924\n",
      "0.353101909160614\n",
      "0.5196104645729065\n",
      "0.3312126398086548\n",
      "0.6327303647994995\n",
      "0.41540250182151794\n",
      "0.4634592533111572\n",
      "0.23342365026474\n",
      "0.23014575242996216\n",
      "0.3256096839904785\n",
      "0.23146523535251617\n",
      "0.22297662496566772\n",
      "0.3444472551345825\n",
      "0.5246215462684631\n",
      "0.27601656317710876\n",
      "0.3494918942451477\n",
      "0.3466914892196655\n",
      "0.16924282908439636\n",
      "0.7262959480285645\n",
      "0.25151145458221436\n",
      "0.6574181318283081\n",
      "0.6268894672393799\n",
      "0.16123943030834198\n",
      "0.2786782681941986\n",
      "0.3472071588039398\n",
      "0.37306225299835205\n",
      "0.38612592220306396\n",
      "0.47501295804977417\n",
      "0.4481768012046814\n",
      "0.2852343022823334\n",
      "0.3428400456905365\n",
      "0.2780252695083618\n",
      "0.45937225222587585\n",
      "0.1187170073390007\n",
      "0.2196587324142456\n",
      "0.16601336002349854\n",
      "0.172430157661438\n",
      "0.35685449838638306\n",
      "0.15870042145252228\n",
      "0.415724515914917\n",
      "0.14616838097572327\n",
      "0.3302113115787506\n",
      "0.4337455630302429\n",
      "0.38934361934661865\n",
      "0.257668673992157\n",
      "0.2534635066986084\n",
      "0.46519893407821655\n",
      "0.2153680920600891\n",
      "0.39628762006759644\n",
      "0.39406752586364746\n",
      "0.4145878553390503\n",
      "0.4828963279724121\n",
      "0.288581907749176\n",
      "0.21710169315338135\n",
      "0.16523899137973785\n",
      "0.2126920521259308\n",
      "0.2770872414112091\n",
      "0.38202598690986633\n",
      "0.5206916928291321\n",
      "0.33741623163223267\n",
      "0.5214543342590332\n",
      "0.49024221301078796\n",
      "0.3730480968952179\n",
      "0.2907922863960266\n",
      "0.24494078755378723\n",
      "0.09812440723180771\n",
      "0.36645135283470154\n",
      "0.6522674560546875\n",
      "0.3376246988773346\n",
      "0.45182275772094727\n",
      "0.27364468574523926\n",
      "0.22650456428527832\n",
      "0.5134497880935669\n",
      "0.4057243764400482\n",
      "0.4056885838508606\n",
      "0.3591401278972626\n",
      "0.4014551043510437\n",
      "0.4772021174430847\n",
      "0.2916612923145294\n",
      "0.5204460620880127\n",
      "0.6414198875427246\n",
      "0.3852959871292114\n",
      "0.5021603107452393\n",
      "0.16295188665390015\n",
      "0.1691248118877411\n",
      "0.2815411686897278\n",
      "0.13884779810905457\n",
      "0.41737112402915955\n",
      "0.11028753966093063\n",
      "0.21893653273582458\n",
      "0.25622671842575073\n",
      "0.3140380382537842\n",
      "0.1933613121509552\n",
      "0.24172475934028625\n",
      "0.30800119042396545\n",
      "0.43454769253730774\n",
      "0.361550509929657\n",
      "0.319253534078598\n",
      "0.3995153307914734\n",
      "0.16372421383857727\n",
      "0.6290668845176697\n",
      "0.23609447479248047\n",
      "0.3715764880180359\n",
      "0.365969181060791\n",
      "0.17295143008232117\n",
      "0.4107864499092102\n",
      "0.4415159821510315\n",
      "0.21404434740543365\n",
      "0.3003323972225189\n",
      "0.13412675261497498\n",
      "0.14134204387664795\n",
      "0.20307068526744843\n",
      "0.5997506976127625\n",
      "0.2604351043701172\n",
      "0.1663089543581009\n",
      "0.32300516963005066\n",
      "0.2205328345298767\n",
      "0.3065777122974396\n",
      "0.16699087619781494\n",
      "0.17673039436340332\n",
      "0.24068734049797058\n",
      "0.5854278802871704\n",
      "0.6591787338256836\n",
      "0.4630710482597351\n",
      "0.2629575729370117\n",
      "0.4451271891593933\n",
      "0.25624048709869385\n",
      "0.20794273912906647\n",
      "0.12422250956296921\n",
      "0.3169119954109192\n",
      "0.7044633030891418\n",
      "0.5792596340179443\n",
      "0.5724735260009766\n",
      "0.29536914825439453\n",
      "0.214523583650589\n",
      "0.33820071816444397\n",
      "0.617423415184021\n",
      "0.23436683416366577\n",
      "0.41458192467689514\n",
      "0.27951323986053467\n",
      "0.21728278696537018\n",
      "0.4664795994758606\n",
      "0.32280582189559937\n",
      "0.37678077816963196\n",
      "0.13182398676872253\n",
      "0.19774359464645386\n",
      "0.4685083031654358\n",
      "0.21898549795150757\n",
      "0.4306240379810333\n",
      "0.35024023056030273\n",
      "0.21225965023040771\n",
      "0.32277530431747437\n",
      "0.5537610054016113\n",
      "0.39873844385147095\n",
      "0.28360703587532043\n",
      "0.6039494872093201\n",
      "0.636913537979126\n",
      "0.09142755717039108\n",
      "0.5438621640205383\n",
      "0.3811855614185333\n",
      "0.687294602394104\n",
      "0.23958024382591248\n",
      "0.3023911714553833\n",
      "0.3463147282600403\n",
      "0.18268996477127075\n",
      "0.2383270263671875\n",
      "0.20742124319076538\n",
      "0.2735995650291443\n",
      "0.11659451574087143\n",
      "0.2127462923526764\n",
      "0.2737113833427429\n",
      "0.33227676153182983\n",
      "0.1654709279537201\n",
      "0.307597815990448\n",
      "0.1411307156085968\n",
      "0.16131514310836792\n",
      "0.3415282666683197\n",
      "0.1399853527545929\n",
      "0.11658114939928055\n",
      "0.1991156041622162\n",
      "0.4195399880409241\n",
      "0.2754056453704834\n",
      "0.34509944915771484\n",
      "0.22245928645133972\n",
      "0.21115100383758545\n",
      "0.2543458044528961\n",
      "0.40343326330184937\n",
      "0.23772186040878296\n",
      "0.1532374918460846\n",
      "0.3385302722454071\n",
      "0.30325984954833984\n",
      "0.23431694507598877\n",
      "0.21715199947357178\n",
      "0.25573381781578064\n",
      "0.2517370581626892\n",
      "0.11920170485973358\n",
      "0.5002409219741821\n",
      "0.4172871708869934\n",
      "0.3572554290294647\n",
      "0.13589243590831757\n",
      "0.11957525461912155\n",
      "0.3000773787498474\n",
      "0.4250601530075073\n",
      "0.4020944833755493\n",
      "0.34906041622161865\n",
      "0.3215633034706116\n",
      "0.21792015433311462\n",
      "0.23996680974960327\n",
      "0.21684683859348297\n",
      "0.2595607340335846\n",
      "0.1132354587316513\n",
      "0.36842840909957886\n",
      "0.21232283115386963\n",
      "0.30774590373039246\n",
      "0.30376264452934265\n",
      "0.17094382643699646\n",
      "0.501509964466095\n",
      "0.28313493728637695\n",
      "0.25677821040153503\n",
      "0.6232077479362488\n",
      "0.20915301144123077\n",
      "0.35433560609817505\n",
      "0.262637197971344\n",
      "0.23640111088752747\n",
      "0.10295258462429047\n",
      "0.3697112798690796\n",
      "0.2501336932182312\n",
      "0.3088389039039612\n",
      "0.2906164824962616\n",
      "0.21903640031814575\n",
      "0.35518574714660645\n",
      "0.278647780418396\n",
      "0.24169227480888367\n",
      "0.1525164395570755\n",
      "0.3342951536178589\n",
      "0.1295909732580185\n",
      "0.19497109949588776\n",
      "0.13333728909492493\n",
      "0.1582045704126358\n",
      "0.2617208957672119\n",
      "0.3794509470462799\n",
      "0.1693868488073349\n",
      "0.20630353689193726\n",
      "0.21085312962532043\n",
      "0.165926992893219\n",
      "0.3875897526741028\n",
      "0.4329841136932373\n",
      "0.46610599756240845\n",
      "0.5766842365264893\n",
      "0.21745535731315613\n",
      "0.3503803014755249\n",
      "0.37448304891586304\n",
      "0.4340667426586151\n",
      "0.44122782349586487\n",
      "0.18986749649047852\n",
      "0.2044728547334671\n",
      "0.3155369162559509\n",
      "0.21246734261512756\n",
      "0.3116472661495209\n",
      "0.24906031787395477\n",
      "0.260357528924942\n",
      "0.4831738770008087\n",
      "0.40684500336647034\n",
      "0.24351823329925537\n",
      "0.21815556287765503\n",
      "0.4166276454925537\n",
      "0.17168259620666504\n",
      "0.5740299224853516\n",
      "0.33227673172950745\n",
      "0.15949565172195435\n",
      "0.31500253081321716\n",
      "0.5367847681045532\n",
      "0.2550697922706604\n",
      "0.25542765855789185\n",
      "0.5363283753395081\n",
      "0.7893590331077576\n",
      "0.5139417052268982\n",
      "0.5498473048210144\n",
      "0.40215668082237244\n",
      "0.24118801951408386\n",
      "0.14556629955768585\n",
      "0.4219321012496948\n",
      "0.2361220270395279\n",
      "0.3893401622772217\n",
      "0.3219209313392639\n",
      "0.3582223057746887\n",
      "0.5105559825897217\n",
      "0.2547028660774231\n",
      "0.3726346492767334\n",
      "0.19586092233657837\n",
      "0.08924409002065659\n",
      "0.43908825516700745\n",
      "0.11387302726507187\n",
      "0.17465916275978088\n",
      "0.25400757789611816\n",
      "0.3416668474674225\n",
      "0.18427926301956177\n",
      "0.22155185043811798\n",
      "0.4983784854412079\n",
      "0.23026272654533386\n",
      "0.21332904696464539\n",
      "0.1471225470304489\n",
      "0.20605716109275818\n",
      "0.09409914165735245\n",
      "0.12737450003623962\n",
      "0.21594929695129395\n",
      "0.4055183529853821\n",
      "0.28256434202194214\n",
      "0.2436944544315338\n",
      "0.21345897018909454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26664999127388\n",
      "0.2394067347049713\n",
      "0.16238558292388916\n",
      "0.1347438395023346\n",
      "0.33366042375564575\n",
      "0.1503206044435501\n",
      "0.20113104581832886\n",
      "0.2354113608598709\n",
      "0.2770374119281769\n",
      "0.5578001141548157\n",
      "0.4311559498310089\n",
      "0.5623863935470581\n",
      "0.45028117299079895\n",
      "0.1918216347694397\n",
      "0.32940223813056946\n",
      "0.2069171816110611\n",
      "0.21587443351745605\n",
      "0.3203088641166687\n",
      "0.2164478600025177\n",
      "0.4744015634059906\n",
      "0.2652137279510498\n",
      "0.2991475760936737\n",
      "0.3499477505683899\n",
      "0.30651456117630005\n",
      "0.40361297130584717\n",
      "0.6256300806999207\n",
      "0.6591168642044067\n",
      "0.5079219341278076\n",
      "0.35079094767570496\n",
      "0.20021586120128632\n",
      "0.2511316239833832\n",
      "0.529048502445221\n",
      "0.5785512924194336\n",
      "0.4632658064365387\n",
      "0.3956384062767029\n",
      "0.2484542578458786\n",
      "0.2843848466873169\n",
      "0.26315492391586304\n",
      "0.30728742480278015\n",
      "0.19801899790763855\n",
      "0.3833433985710144\n",
      "0.12003475427627563\n",
      "0.450297474861145\n",
      "0.4171110987663269\n",
      "0.20196205377578735\n",
      "0.37207505106925964\n",
      "0.29704344272613525\n",
      "0.6132158041000366\n",
      "0.4879414439201355\n",
      "0.3790995478630066\n",
      "0.3444272577762604\n",
      "0.3305028975009918\n",
      "0.3999558389186859\n",
      "0.7761138677597046\n",
      "0.3163827955722809\n",
      "0.26366472244262695\n",
      "0.38996395468711853\n",
      "0.4135761260986328\n",
      "0.4259216785430908\n",
      "0.5772121548652649\n",
      "0.6755984425544739\n",
      "0.2666073441505432\n",
      "0.22496375441551208\n",
      "0.21435853838920593\n",
      "0.2248164415359497\n",
      "0.5482181906700134\n",
      "0.3109802007675171\n",
      "0.26221907138824463\n",
      "0.22593820095062256\n",
      "0.17634384334087372\n",
      "0.25110357999801636\n",
      "0.30998697876930237\n",
      "0.24550358951091766\n",
      "0.20090633630752563\n",
      "0.34215256571769714\n",
      "0.36548757553100586\n",
      "0.28992363810539246\n",
      "0.2311067283153534\n",
      "0.6233904361724854\n",
      "0.484784871339798\n",
      "0.21429182589054108\n",
      "0.15376350283622742\n",
      "0.23359563946723938\n",
      "0.26404744386672974\n",
      "0.2776540517807007\n",
      "0.1850680708885193\n",
      "0.30130353569984436\n",
      "0.4571628272533417\n",
      "0.2996063232421875\n",
      "0.3631052076816559\n",
      "0.25619181990623474\n",
      "0.366942435503006\n",
      "0.25372907519340515\n",
      "0.26718226075172424\n",
      "0.3602583706378937\n",
      "0.34153008460998535\n",
      "0.28967705368995667\n",
      "0.3386286199092865\n",
      "0.40527111291885376\n",
      "0.14226137101650238\n",
      "0.30338916182518005\n",
      "0.236416757106781\n",
      "0.40342336893081665\n",
      "0.5279560089111328\n",
      "0.23173001408576965\n",
      "0.36683523654937744\n",
      "0.3010121285915375\n",
      "0.4759528636932373\n",
      "0.2630288004875183\n",
      "0.2333550751209259\n",
      "0.2592806816101074\n",
      "0.2873677909374237\n",
      "0.15789969265460968\n",
      "0.1381281614303589\n",
      "0.21475954353809357\n",
      "0.25006234645843506\n",
      "0.2742111086845398\n",
      "0.31753867864608765\n",
      "0.24301382899284363\n",
      "0.41013166308403015\n",
      "0.19489678740501404\n",
      "0.36000165343284607\n",
      "0.13437935709953308\n",
      "0.32688984274864197\n",
      "0.1619132161140442\n",
      "0.181458979845047\n",
      "0.3156425654888153\n",
      "0.30713552236557007\n",
      "0.2214461714029312\n",
      "0.34509944915771484\n",
      "0.21659564971923828\n",
      "0.28261107206344604\n",
      "0.29522305727005005\n",
      "0.3173983097076416\n",
      "0.3420479893684387\n",
      "0.23262494802474976\n",
      "0.13870571553707123\n",
      "0.09496763348579407\n",
      "0.4283670485019684\n",
      "0.16177569329738617\n",
      "0.27754518389701843\n",
      "0.3192886412143707\n",
      "0.33056122064590454\n",
      "0.19337263703346252\n",
      "0.518927276134491\n",
      "0.2037525773048401\n",
      "0.27174949645996094\n",
      "0.1650601625442505\n",
      "0.19538438320159912\n",
      "0.18988800048828125\n",
      "0.40358102321624756\n",
      "0.2714206874370575\n",
      "0.5111941695213318\n",
      "0.36061975359916687\n",
      "0.3985305726528168\n",
      "0.6142760515213013\n",
      "0.7001543045043945\n",
      "0.40222859382629395\n",
      "0.3715727627277374\n",
      "0.2829219698905945\n",
      "0.2919601500034332\n",
      "0.15637212991714478\n",
      "0.28457650542259216\n",
      "0.3050665259361267\n",
      "0.09200496226549149\n",
      "0.21092589199543\n",
      "0.5220014452934265\n",
      "0.26255765557289124\n",
      "0.24334248900413513\n",
      "0.08612474054098129\n",
      "0.10515687614679337\n",
      "0.2662796974182129\n",
      "0.3465649485588074\n",
      "0.10768754035234451\n",
      "0.361334890127182\n",
      "0.408426970243454\n",
      "0.4795825481414795\n",
      "0.29091235995292664\n",
      "0.07476626336574554\n",
      "0.25458118319511414\n",
      "0.531929612159729\n",
      "0.6663756966590881\n",
      "0.45008137822151184\n",
      "0.25680384039878845\n",
      "0.25676581263542175\n",
      "0.2517170011997223\n",
      "0.21807852387428284\n",
      "0.09094813466072083\n",
      "0.26212242245674133\n",
      "0.4320497512817383\n",
      "0.3321293592453003\n",
      "0.3226647973060608\n",
      "0.12495885044336319\n",
      "0.1984308809041977\n",
      "0.2275298535823822\n",
      "0.32712775468826294\n",
      "0.22660742700099945\n",
      "0.2447812259197235\n",
      "0.2344897985458374\n",
      "0.5371009707450867\n",
      "0.17854607105255127\n",
      "0.36522242426872253\n",
      "0.2978374660015106\n",
      "0.46247148513793945\n",
      "0.3378945589065552\n",
      "0.21106866002082825\n",
      "0.19145309925079346\n",
      "0.315510630607605\n",
      "0.4133884906768799\n",
      "0.20010221004486084\n",
      "0.22879059612751007\n",
      "0.3079529106616974\n",
      "0.42023998498916626\n",
      "0.3326680362224579\n",
      "0.3869493901729584\n",
      "0.29121237993240356\n",
      "0.4695686995983124\n",
      "0.2674703299999237\n",
      "0.349099338054657\n",
      "0.29184162616729736\n",
      "0.10841159522533417\n",
      "0.30525824427604675\n",
      "0.28510627150535583\n",
      "0.2323872298002243\n",
      "0.4733445644378662\n",
      "0.1020587757229805\n",
      "0.09228511899709702\n",
      "0.2354910969734192\n",
      "0.3274787664413452\n",
      "0.2105308175086975\n",
      "0.2285689115524292\n",
      "0.227408766746521\n",
      "0.41946834325790405\n",
      "0.16648714244365692\n",
      "0.22454681992530823\n",
      "0.2048269659280777\n",
      "0.23095984756946564\n",
      "0.17298591136932373\n",
      "0.17373743653297424\n",
      "0.31106987595558167\n",
      "0.28394705057144165\n",
      "0.58447265625\n",
      "0.3334415555000305\n",
      "0.37373173236846924\n",
      "0.3496677577495575\n",
      "0.5774134993553162\n",
      "0.6060674786567688\n",
      "0.37860333919525146\n",
      "0.3688547909259796\n",
      "0.48381102085113525\n",
      "0.11060428619384766\n",
      "0.16597002744674683\n",
      "0.2197476476430893\n",
      "0.7247437238693237\n",
      "0.3889686167240143\n",
      "0.16335321962833405\n",
      "0.2761790156364441\n",
      "0.28639745712280273\n",
      "0.3783947825431824\n",
      "0.22621622681617737\n",
      "0.33012253046035767\n",
      "0.3000677824020386\n",
      "0.21153391897678375\n",
      "0.3392728567123413\n",
      "0.2337477207183838\n",
      "0.15285398066043854\n",
      "0.18493136763572693\n",
      "0.14757780730724335\n",
      "0.2881753742694855\n",
      "0.2552618682384491\n",
      "0.21673977375030518\n",
      "0.10917754471302032\n",
      "0.7068987488746643\n",
      "0.18915998935699463\n",
      "0.3352542519569397\n",
      "0.3261713981628418\n",
      "0.8305209875106812\n",
      "0.25115251541137695\n",
      "0.21855071187019348\n",
      "0.17383238673210144\n",
      "0.12588787078857422\n",
      "0.2958681881427765\n",
      "0.13162708282470703\n",
      "0.3349229693412781\n",
      "0.4077727794647217\n",
      "0.38528507947921753\n",
      "0.2913265824317932\n",
      "0.25949573516845703\n",
      "0.3044013977050781\n",
      "0.18760034441947937\n",
      "0.2432534098625183\n",
      "0.250101238489151\n",
      "0.2788432538509369\n",
      "0.43775078654289246\n",
      "0.21221062541007996\n",
      "0.519285261631012\n",
      "0.43213966488838196\n",
      "0.4328325390815735\n",
      "0.3254127502441406\n",
      "0.3654133975505829\n",
      "0.4194742441177368\n",
      "0.3699948489665985\n",
      "0.2151639759540558\n",
      "0.16216978430747986\n",
      "0.26801997423171997\n",
      "0.3494148254394531\n",
      "0.2618037462234497\n",
      "0.43559083342552185\n",
      "0.32564398646354675\n",
      "0.34742844104766846\n",
      "0.2825170159339905\n",
      "0.4301983416080475\n",
      "0.24776431918144226\n",
      "0.3896659314632416\n",
      "0.32634031772613525\n",
      "0.5110913515090942\n",
      "0.5552268624305725\n",
      "0.35081928968429565\n",
      "0.7257660031318665\n",
      "0.5295454859733582\n",
      "0.37930452823638916\n",
      "0.13992305099964142\n",
      "0.2855084538459778\n",
      "0.4323386251926422\n",
      "0.602631688117981\n",
      "0.35896074771881104\n",
      "0.2760676145553589\n",
      "0.26752525568008423\n",
      "0.3406349718570709\n",
      "0.35103142261505127\n",
      "0.3245531916618347\n",
      "0.43669769167900085\n",
      "0.31648242473602295\n",
      "0.45050299167633057\n",
      "0.4383738040924072\n",
      "0.433847576379776\n",
      "0.39830905199050903\n",
      "0.2080235332250595\n",
      "0.10320873558521271\n",
      "0.45504286885261536\n",
      "0.2603672742843628\n",
      "0.5271704196929932\n",
      "0.2609326243400574\n",
      "0.3582462966442108\n",
      "0.21250608563423157\n",
      "0.25467759370803833\n",
      "0.1997344195842743\n",
      "0.18429771065711975\n",
      "0.28004521131515503\n",
      "0.3969537019729614\n",
      "0.36683389544487\n",
      "0.3301028907299042\n",
      "0.27592140436172485\n",
      "0.26267939805984497\n",
      "0.15398047864437103\n",
      "0.158904567360878\n",
      "0.35353320837020874\n",
      "0.11259279400110245\n",
      "0.08622719347476959\n",
      "0.14459477365016937\n",
      "0.17270199954509735\n",
      "0.25508761405944824\n",
      "0.16989627480506897\n",
      "0.1953660547733307\n",
      "0.1822161078453064\n",
      "0.16705016791820526\n",
      "0.308231920003891\n",
      "0.41964229941368103\n",
      "0.5041699409484863\n",
      "0.29683414101600647\n",
      "0.4372158646583557\n",
      "0.3683531582355499\n",
      "0.353401243686676\n",
      "0.48072680830955505\n",
      "0.2845206558704376\n",
      "0.1608724296092987\n",
      "0.29149430990219116\n",
      "0.16880649328231812\n",
      "0.25835150480270386\n",
      "0.32574668526649475\n",
      "0.18077711760997772\n",
      "0.18970909714698792\n",
      "0.2825632095336914\n",
      "0.46401843428611755\n",
      "0.16704821586608887\n",
      "0.3033909201622009\n",
      "0.22182397544384003\n",
      "0.1640421599149704\n",
      "0.13265439867973328\n",
      "0.2408934086561203\n",
      "0.27419528365135193\n",
      "0.31302812695503235\n",
      "0.36816850304603577\n",
      "0.27705761790275574\n",
      "0.25247061252593994\n",
      "0.24212056398391724\n",
      "0.10001251101493835\n",
      "0.28599804639816284\n",
      "0.37623175978660583\n",
      "0.17196476459503174\n",
      "0.2417687475681305\n",
      "0.1331978440284729\n",
      "0.17390203475952148\n",
      "0.312989205121994\n",
      "0.2750047445297241\n",
      "0.2720172703266144\n",
      "0.1536215841770172\n",
      "0.2735505700111389\n",
      "0.4320748448371887\n",
      "0.442812442779541\n",
      "0.23794132471084595\n",
      "0.5954413414001465\n",
      "0.6853622198104858\n",
      "0.9394363164901733\n",
      "0.5437069535255432\n",
      "0.4787881672382355\n",
      "0.21944597363471985\n",
      "0.27616143226623535\n",
      "0.4898083806037903\n",
      "0.2672984302043915\n",
      "0.3423917889595032\n",
      "0.2676337957382202\n",
      "0.40876758098602295\n",
      "0.27559903264045715\n",
      "0.18901434540748596\n",
      "0.15244397521018982\n",
      "0.3813318610191345\n",
      "0.3069840371608734\n",
      "0.33046483993530273\n",
      "0.22787277400493622\n",
      "0.34355807304382324\n",
      "0.391947865486145\n",
      "0.4518418312072754\n",
      "0.3345831632614136\n",
      "0.2756046950817108\n",
      "0.14995712041854858\n",
      "0.15101811289787292\n",
      "0.18164914846420288\n",
      "0.2063142955303192\n",
      "0.14962664246559143\n",
      "0.2790414094924927\n",
      "0.2234373539686203\n",
      "0.26632416248321533\n",
      "0.8080949783325195\n",
      "0.4023231267929077\n",
      "0.5217785239219666\n",
      "0.2491297423839569\n",
      "0.22071720659732819\n",
      "0.315356969833374\n",
      "0.2539149224758148\n",
      "0.3200138211250305\n",
      "0.2166031301021576\n",
      "0.266964852809906\n",
      "0.43763330578804016\n",
      "0.20822560787200928\n",
      "0.3489786386489868\n",
      "0.27229738235473633\n",
      "0.16873720288276672\n",
      "0.2792781591415405\n",
      "0.28572869300842285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20137378573417664\n",
      "0.37234604358673096\n",
      "0.31412473320961\n",
      "0.26681235432624817\n",
      "0.2734246253967285\n",
      "0.37129080295562744\n",
      "0.17901091277599335\n",
      "0.18156608939170837\n",
      "0.2055858075618744\n",
      "0.4789082407951355\n",
      "0.24652770161628723\n",
      "0.27816295623779297\n",
      "0.2209993302822113\n",
      "0.3413044512271881\n",
      "0.5025866031646729\n",
      "0.31320512294769287\n",
      "0.6157188415527344\n",
      "0.3799436092376709\n",
      "0.40831446647644043\n",
      "0.2013695240020752\n",
      "0.19731922447681427\n",
      "0.2783111035823822\n",
      "0.20887354016304016\n",
      "0.19431942701339722\n",
      "0.3245672881603241\n",
      "0.5057836771011353\n",
      "0.24403473734855652\n",
      "0.32681384682655334\n",
      "0.31411057710647583\n",
      "0.1472833901643753\n",
      "0.6744614243507385\n",
      "0.255066454410553\n",
      "0.6679619550704956\n",
      "0.6264370083808899\n",
      "0.1491003930568695\n",
      "0.24817582964897156\n",
      "0.3436248004436493\n",
      "0.3603939116001129\n",
      "0.3900687098503113\n",
      "0.4272272288799286\n",
      "0.4074840545654297\n",
      "0.2616318464279175\n",
      "0.3075196146965027\n",
      "0.28255343437194824\n",
      "0.4408625364303589\n",
      "0.09109967201948166\n",
      "0.22258135676383972\n",
      "0.13801100850105286\n",
      "0.14245744049549103\n",
      "0.3510485589504242\n",
      "0.1265958994626999\n",
      "0.372012734413147\n",
      "0.12087798118591309\n",
      "0.2780247628688812\n",
      "0.39886292815208435\n",
      "0.3702555298805237\n",
      "0.2532743811607361\n",
      "0.1990768015384674\n",
      "0.42411646246910095\n",
      "0.17395569384098053\n",
      "0.3529008626937866\n",
      "0.3781527876853943\n",
      "0.3725413382053375\n",
      "0.4767059087753296\n",
      "0.2513159215450287\n",
      "0.1931680291891098\n",
      "0.161935955286026\n",
      "0.19379012286663055\n",
      "0.23479801416397095\n",
      "0.3638353645801544\n",
      "0.5116238594055176\n",
      "0.31889551877975464\n",
      "0.500432014465332\n",
      "0.4503767490386963\n",
      "0.35302141308784485\n",
      "0.2611919045448303\n",
      "0.2083471715450287\n",
      "0.08025117218494415\n",
      "0.3343338370323181\n",
      "0.6122084856033325\n",
      "0.304790735244751\n",
      "0.45251768827438354\n",
      "0.2467757612466812\n",
      "0.2005012333393097\n",
      "0.5053774118423462\n",
      "0.373747855424881\n",
      "0.37254512310028076\n",
      "0.32315197587013245\n",
      "0.35950738191604614\n",
      "0.4473118185997009\n",
      "0.2596762180328369\n",
      "0.5165090560913086\n",
      "0.6188409924507141\n",
      "0.36887258291244507\n",
      "0.47554948925971985\n",
      "0.1340596079826355\n",
      "0.14718970656394958\n",
      "0.2553836703300476\n",
      "0.11678992956876755\n",
      "0.4288584291934967\n",
      "0.09260155260562897\n",
      "0.211623877286911\n",
      "0.23709315061569214\n",
      "0.2842642366886139\n",
      "0.1774437129497528\n",
      "0.2155095934867859\n",
      "0.28285956382751465\n",
      "0.42023569345474243\n",
      "0.3061128556728363\n",
      "0.2782096266746521\n",
      "0.3996448218822479\n",
      "0.14144384860992432\n",
      "0.6199020147323608\n",
      "0.202953040599823\n",
      "0.36398056149482727\n",
      "0.3178555369377136\n",
      "0.16735264658927917\n",
      "0.3846661150455475\n",
      "0.4331952631473541\n",
      "0.18982577323913574\n",
      "0.27758413553237915\n",
      "0.10798070579767227\n",
      "0.1152302697300911\n",
      "0.1854410171508789\n",
      "0.5989858508110046\n",
      "0.2194317877292633\n",
      "0.15690797567367554\n",
      "0.28324881196022034\n",
      "0.19624292850494385\n",
      "0.2899574041366577\n",
      "0.15816468000411987\n",
      "0.16390126943588257\n",
      "0.21807745099067688\n",
      "0.5690503120422363\n",
      "0.6559155583381653\n",
      "0.420651912689209\n",
      "0.21969428658485413\n",
      "0.4022149443626404\n",
      "0.2437344193458557\n",
      "0.20003673434257507\n",
      "0.10849341750144958\n",
      "0.28048408031463623\n",
      "0.6746637225151062\n",
      "0.55644690990448\n",
      "0.5505109429359436\n",
      "0.26632410287857056\n",
      "0.20003266632556915\n",
      "0.31518852710723877\n",
      "0.625076413154602\n",
      "0.2068101316690445\n",
      "0.32011088728904724\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "lr = 0.5\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "#         set_trace()\n",
    "#         注意这里没有batch的设置，但是在使用dataset refactor的时候可以使用batch\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i: end_i]\n",
    "        yb = y_train[start_i: end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        print(loss.item())\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0831, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0831, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn.Module\n",
    "这个module与tf.keras.Model 有异曲同工之妙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MnistLogistic(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         nn.parameter 是一个Tensor的subclass，当与nn.Module一起使用的时候，能自动嵌入到nn.Module中\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10)/math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistLogistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3094, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "#                 注意区别，对于一个model，那就是model.zero_grad()\n",
    "#                 而对于每个Tensor，那就是Tensor.grad.zero_() in-place zero\n",
    "                model.zero_grad()\n",
    "fit()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0805, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2839, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0820, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2942, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0813, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs +1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "#         用了optim以后就不需要with torch.no_grad()和model.zero_grad()了\n",
    "#         with torch.no_grad()只用于validation 以及 inference\n",
    "        opt.zero_grad()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 784]) torch.Size([50000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warpping Tensors. Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "# 会自动将若干输入的Tensor沿着first dimension 打包成 tuple，所以所有Tensor.size(0)都会像等\n",
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0833, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1) // bs + 1):\n",
    "        xb, yb = train_ds[i*bs: i*bs+bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader相比dataset的好处是增加了自动mini-batch, shuffle以及multi-thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0810, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "#     虽然现在在前面多了一层batch dimension，但是可以用xb, yb in train_dl这样的iteration来处理，PyTorch能自动处理\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "val_ds = TensorDataset(x_val, y_val)\n",
    "val_dl = DataLoader(val_ds, batch_size=bs*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3309369147578372\n",
      "1 0.3421630617938464\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "for epoch in range(epochs):\n",
    "#   在training前使用model.train() 在inference前使用model.eval() 能改变model中dropout和batch_norm的行为\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "#   在inference前使用model.eval()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "#         这里的sum能够承接多个对象\n",
    "        val_loss = sum(loss_func(model(xb), yb) for xb, yb in val_dl)\n",
    "        \n",
    "    print(epoch, val_loss.item() / len(val_dl))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个有点像tf中的train batch\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, val_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb, yb in val_dl])\n",
    "            val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, val_ds, bs):\n",
    "    return(\n",
    "    DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "    DataLoader(val_ds, batch_size=bs*2),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.30750353588461876\n",
      "1 0.3110940942108631\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = get_data(train_ds, val_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "#       (28 + padding * 2 -(kernel_size -1))/stride = (28 + 1*2 -(3-1))/2 = 14\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "#       (14 + padding * 2 -(kernel_size -1))/stride = (14 + 1*2 -(3-1))/2 = 7\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "#       (7 + padding * 2 -(kernel_size -1))/stride = (7 + 1*2 -(3-1))/2 = 3.5 = 4\n",
    "        xb = F.avg_pool2d(xb, kernel_size=4)\n",
    "#       avgpooling 2d: (4 - (kernel_size-1))/stride = (4 -(4-1)) / 1 = 1\n",
    "        return xb.view(-1, xb.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 784])\n"
     ]
    }
   ],
   "source": [
    "print(xb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.33377406539916993\n",
      "1 0.3164267322063446\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# 注意这里的loss_func 是 crossentropy，包含log-softmax和negative likelihood loss\n",
    "fit(epochs, model, loss_func, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.sequential and custom layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的custom layer也是跟custom model一样，是nn.Module的subclass，使用sequential model的时候一定要用nn.Module的subclass\n",
    "# 加载进去，不可以直接用lambda function或者是\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x):\n",
    "#         print(x.size())\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "model = nn.Sequential(Lambda(preprocess),\n",
    "                     nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "                     nn.ReLU(),\n",
    "                     nn.AvgPool2d(4),\n",
    "                     Lambda(lambda x: x.view(-1, x.size(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6344017515182495\n",
      "1 0.5646419269561768\n"
     ]
    }
   ],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "fit(epochs, model, loss_func, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "# yield 能得到一个generator对象\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, val_dl = get_data(train_ds, val_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "val_dl = WrappedDataLoader(val_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "                     nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "                     nn.ReLU(),\n",
    "#                    AdaptiveAvgPool2d could return arbitary spatial size of output\n",
    "                     nn.AdaptiveAvgPool2d(1),\n",
    "                     Lambda(lambda x: x.view(x.size(0), -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.49939156489372255\n",
      "1 0.4860512022018433\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将dataloader的所有batch都移动到GPU上\n",
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "train_dl, val_dl = get_data(train_ds, val_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "val_dl = WrappedDataLoader(val_dl, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4594813583850861\n",
      "1 0.42788300681114194\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
