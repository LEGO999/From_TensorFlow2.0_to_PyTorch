{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# leaf variable 就是直接create的Tensor，他们不是其他操作的结果\n",
    "x = torch.ones(2,2, requires_grad=True)\n",
    "# 像x.t_() 这种in-place operation不能在requires_grad=True中使用\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7fbacc4547b8>\n"
     ]
    }
   ],
   "source": [
    "# 所谓的grad_fn 就是创造这个Tensor的function\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<TBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y.t_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 和TF一样，这里的相乘都是elementwise\n",
    "# 假如这里将表达式改成z = y * y.t_() * 3 ，这里不会得到matrix相乘的结果，因为Tensor.t_()是一个in-place operation\n",
    "# 所以在evaluate这个式子数值的时候也会将y改变，在这个expression中y与y.t_()是等效。\n",
    "z = y * y * 3\n",
    "# Tensor.mean() 等效于torch.mean()\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "tensor(4.1259, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x7fba4558b6a0>\n"
     ]
    }
   ],
   "source": [
    "# 不同于torch.rand()，这里生成的是从N(0,1) Gaussian Distribution采样的Tensor\n",
    "a = torch.randn(2,2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "# requires_grad 这个flag by default 是 false的\n",
    "print(a.requires_grad)\n",
    "# Tensor.requires_grad_()能将Tensor.requires_grad这个flag改变，如True，就开始追踪，有grad_fn\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor.backward() 开始backpropagation\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# 求出out对x的导数，因为x为一个矩阵，求出来的导数跟x形状一致\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/backpropagation.png\" alt=\"图片替换文本\" width=\"500\" height=\"713\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5964) tensor([ 2.4776, -0.6099, -0.4804], grad_fn=<MulBackward0>)\n",
      "tensor(5.1929) tensor([ 4.9553, -1.2197, -0.9608], grad_fn=<MulBackward0>)\n",
      "tensor(10.3857) tensor([ 9.9106, -2.4394, -1.9216], grad_fn=<MulBackward0>)\n",
      "tensor(20.7714) tensor([19.8212, -4.8788, -3.8431], grad_fn=<MulBackward0>)\n",
      "tensor(41.5428) tensor([39.6424, -9.7577, -7.6863], grad_fn=<MulBackward0>)\n",
      "tensor(83.0857) tensor([ 79.2847, -19.5154, -15.3726], grad_fn=<MulBackward0>)\n",
      "tensor(166.1714) tensor([158.5694, -39.0308, -30.7451], grad_fn=<MulBackward0>)\n",
      "tensor(332.3428) tensor([317.1389, -78.0615, -61.4903], grad_fn=<MulBackward0>)\n",
      "tensor(664.6855) tensor([ 634.2777, -156.1231, -122.9805], grad_fn=<MulBackward0>)\n",
      "tensor([1268.5554, -312.2461, -245.9610], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "# Tensor.data.norm()等于torch.norm(Tensor)，在默认情况下计算的是Forbenius Norm(matrix) 或者 L2 Norm(Vector)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    print(y.data.norm(), y)\n",
    "    y = y * 2\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 102.4000, 1024.0000,    1.0240])\n"
     ]
    }
   ],
   "source": [
    "# 这里y是一个向量， x也是一个向量，dy/dx应该是一个Jacobian矩阵， 但是若果y与x是更高维的Tensor，这里就无法给出导数的定义，\n",
    "# 所以PyTorch规定，Tensor.backward()，若果backward中没有额外的参数就不能求导，只能求scalar对Tensor的导数\n",
    "# 这里给出一个简单的例子，假设y也是一个中间变量，已知最终结果对y的求导结果为v，求最终结果对于x的求导\n",
    "# 可以理解为(y_transpose * v)得到一个scalar后再到x求导，这里的v可以像上面一个理解为最终结果对y的求导，\n",
    "# 也可以理解为对y各个分量求和的权重\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# x在上一个cell开了requires_grad，所以在它基础计算的Tensor自然也都是开了追踪\n",
    "# 除非有no_grad()暂时把追踪关闭\n",
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# 或者使用Tensor.detach()，返回的值与x一样但是不再具有追踪功能\n",
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "# Tensor0.eq(Tensor1) 与torch.eq()一样，会返回一个elementwise的BoolTensor\n",
    "# BoolTensor.all()会返回另一个boolTensor，如果BoolTensor全部都是True，它也返回一个True,否则只要有一个是False就返回一个False\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
