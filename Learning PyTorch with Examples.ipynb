{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core of PyTorch\n",
    "*  Tensor (similar to Numpy array) computation on GPUs\n",
    "*  Automatic differentation for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下从低级到高级的API让人熟悉PyTorchAPI   \n",
    "1.1 Tensor（运用NumpyArray与手动求导写一个NN）   \n",
    "1.2 PyTorch Tensor（PyTorchTensor与手动求导写一个NN）  \n",
    "1.3 PyTorch Tensor + Autograd (PyTorchTensor与自动求导写一个NN)    \n",
    "1.4 PyTorch Tensor + customized Autograd写一个NN  \n",
    "1.5 PyTorch NN Sequential + Autograd写一个NN  \n",
    "1.6 PyTorch NN + optimizer写一个NN   \n",
    "1.7 PyTorch NN custom + optimizer 写一个NN  \n",
    "1.8 PyTorch NN custom + control flow + optimizer 写一个NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30128081.5812336\n",
      "1 26528397.349934135\n",
      "2 31294229.266981103\n",
      "3 39529371.1059206\n",
      "4 44486568.21986099\n",
      "5 37907919.25611724\n",
      "6 23016883.13894947\n",
      "7 10155988.736892443\n",
      "8 4078198.269220598\n",
      "9 1900117.4871771801\n",
      "10 1155374.1677559717\n",
      "11 851578.6325813667\n",
      "12 688985.101596944\n",
      "13 579323.8730722324\n",
      "14 495455.69504850265\n",
      "15 427704.1255260593\n",
      "16 371669.7160113986\n",
      "17 324668.26757468685\n",
      "18 284922.450027877\n",
      "19 251075.02210418606\n",
      "20 222077.7279341534\n",
      "21 197154.58733523643\n",
      "22 175584.4503152246\n",
      "23 156828.944837175\n",
      "24 140470.82822714548\n",
      "25 126157.11472063104\n",
      "26 113587.6651076428\n",
      "27 102509.53187232028\n",
      "28 92716.15103075396\n",
      "29 84024.85172504946\n",
      "30 76294.12671197455\n",
      "31 69404.76015181348\n",
      "32 63250.60786566926\n",
      "33 57739.00936646465\n",
      "34 52790.565391530836\n",
      "35 48337.976540217824\n",
      "36 44322.75460966771\n",
      "37 40695.76498237411\n",
      "38 37413.34290808212\n",
      "39 34438.865830515395\n",
      "40 31738.965327707665\n",
      "41 29283.67046718638\n",
      "42 27050.437183494618\n",
      "43 25017.701727563002\n",
      "44 23161.20708470529\n",
      "45 21463.49767308954\n",
      "46 19909.10919404295\n",
      "47 18484.486968031666\n",
      "48 17177.916278822064\n",
      "49 15978.32786917094\n",
      "50 14874.676615475542\n",
      "51 13857.458709184602\n",
      "52 12919.899370539586\n",
      "53 12054.427069557732\n",
      "54 11254.70832730626\n",
      "55 10515.684237803882\n",
      "56 9832.602186549124\n",
      "57 9201.535091930493\n",
      "58 8616.99058274915\n",
      "59 8074.448356494906\n",
      "60 7570.350744012334\n",
      "61 7101.650255585573\n",
      "62 6665.401339490289\n",
      "63 6259.141190005957\n",
      "64 5880.615806450028\n",
      "65 5527.562554619339\n",
      "66 5198.090824011728\n",
      "67 4890.340006443505\n",
      "68 4602.716196193591\n",
      "69 4333.7801640932885\n",
      "70 4082.0923149058262\n",
      "71 3846.5177693592664\n",
      "72 3625.799356065432\n",
      "73 3418.9616714077047\n",
      "74 3225.0221449233522\n",
      "75 3043.1144359049276\n",
      "76 2872.328637445784\n",
      "77 2711.9477636823376\n",
      "78 2561.2665363735805\n",
      "79 2419.6620141474127\n",
      "80 2286.5387557302242\n",
      "81 2161.2872862699\n",
      "82 2043.4151256889977\n",
      "83 1932.4625659812464\n",
      "84 1827.9922480231578\n",
      "85 1729.5090202659985\n",
      "86 1636.7098774932967\n",
      "87 1549.2520155744764\n",
      "88 1466.792416200226\n",
      "89 1389.0110879746044\n",
      "90 1315.5836536933562\n",
      "91 1246.284104647227\n",
      "92 1180.8696249807429\n",
      "93 1119.087111997083\n",
      "94 1060.731594310782\n",
      "95 1005.5979084406945\n",
      "96 953.5035164803353\n",
      "97 904.243013492663\n",
      "98 857.6788163758831\n",
      "99 813.6307638656453\n",
      "100 771.9725460277693\n",
      "101 732.5489501883947\n",
      "102 695.2384120906224\n",
      "103 659.9214680785697\n",
      "104 626.4934173281183\n",
      "105 594.8300843740344\n",
      "106 564.8430904088132\n",
      "107 536.4439737928476\n",
      "108 509.5342114276733\n",
      "109 484.02860135102645\n",
      "110 459.86003036414525\n",
      "111 436.9483288631125\n",
      "112 415.22648055262414\n",
      "113 394.62454560075\n",
      "114 375.0878738051889\n",
      "115 356.5629962966287\n",
      "116 338.98410957084286\n",
      "117 322.3012067115402\n",
      "118 306.46954325686306\n",
      "119 291.4489206827921\n",
      "120 277.18813383931206\n",
      "121 263.6485987862434\n",
      "122 250.7949207865376\n",
      "123 238.59438992635515\n",
      "124 227.00690301789638\n",
      "125 215.9985013584811\n",
      "126 205.54505658020216\n",
      "127 195.6128941852557\n",
      "128 186.17413310241818\n",
      "129 177.2065725596253\n",
      "130 168.68520055498436\n",
      "131 160.58530155827526\n",
      "132 152.88814286596948\n",
      "133 145.5686868393077\n",
      "134 138.61098228609671\n",
      "135 131.99454612255596\n",
      "136 125.7026623474583\n",
      "137 119.72086773235566\n",
      "138 114.03245401406302\n",
      "139 108.6194841403415\n",
      "140 103.47163647074139\n",
      "141 98.5749802550886\n",
      "142 93.91435595230321\n",
      "143 89.47995926665689\n",
      "144 85.26127267261495\n",
      "145 81.24632414608749\n",
      "146 77.42473592840481\n",
      "147 73.7876063887939\n",
      "148 70.32740588967162\n",
      "149 67.0319585953662\n",
      "150 63.89460127758714\n",
      "151 60.90748569457244\n",
      "152 58.06385578590819\n",
      "153 55.355378285613185\n",
      "154 52.776102551646076\n",
      "155 50.320749979457545\n",
      "156 47.9816924274954\n",
      "157 45.75338986650465\n",
      "158 43.631212203379214\n",
      "159 41.60957661511062\n",
      "160 39.6836757759361\n",
      "161 37.84867963559865\n",
      "162 36.09589321039749\n",
      "163 34.42620660167739\n",
      "164 32.83522785414911\n",
      "165 31.31958025846862\n",
      "166 29.87547361356578\n",
      "167 28.49924158167417\n",
      "168 27.187732587959133\n",
      "169 25.938098171383817\n",
      "170 24.746828411381017\n",
      "171 23.613054138324102\n",
      "172 22.533443541818958\n",
      "173 21.504130659742962\n",
      "174 20.522727998583402\n",
      "175 19.587007338565908\n",
      "176 18.694995434863216\n",
      "177 17.84418219245152\n",
      "178 17.03305963547863\n",
      "179 16.25963860266303\n",
      "180 15.52174605704544\n",
      "181 14.818014134735096\n",
      "182 14.146844439999729\n",
      "183 13.506661255303868\n",
      "184 12.895930878640144\n",
      "185 12.313346347307693\n",
      "186 11.757805274098871\n",
      "187 11.227473783198867\n",
      "188 10.72149281169059\n",
      "189 10.23885225080709\n",
      "190 9.77824302526248\n",
      "191 9.338678339187618\n",
      "192 8.919240752702658\n",
      "193 8.519103303223353\n",
      "194 8.13710908648953\n",
      "195 7.772491875774984\n",
      "196 7.424555520679823\n",
      "197 7.092365645318146\n",
      "198 6.775267510525836\n",
      "199 6.472621547538294\n",
      "200 6.183769357758604\n",
      "201 5.908074268488857\n",
      "202 5.6446993007751125\n",
      "203 5.393320930848217\n",
      "204 5.153262390745473\n",
      "205 4.924029656459528\n",
      "206 4.705196096495641\n",
      "207 4.496257989283116\n",
      "208 4.296674478286104\n",
      "209 4.106071161411648\n",
      "210 3.9240950093832723\n",
      "211 3.7502696364655166\n",
      "212 3.584239021924086\n",
      "213 3.425709834368106\n",
      "214 3.274286202588633\n",
      "215 3.1296117054718735\n",
      "216 2.9914308388581783\n",
      "217 2.859460990923715\n",
      "218 2.7333541043488214\n",
      "219 2.6128780801656193\n",
      "220 2.4978307782768194\n",
      "221 2.3879072838112148\n",
      "222 2.282851694583836\n",
      "223 2.182475427124898\n",
      "224 2.086594208293775\n",
      "225 1.9949572839076206\n",
      "226 1.907389410786473\n",
      "227 1.8237574195572024\n",
      "228 1.7438076486100975\n",
      "229 1.6674074836612651\n",
      "230 1.5943848533860443\n",
      "231 1.524619307853455\n",
      "232 1.4579253764626874\n",
      "233 1.394189746368453\n",
      "234 1.3332818302410074\n",
      "235 1.2750661805890053\n",
      "236 1.219470914008809\n",
      "237 1.1663230914349545\n",
      "238 1.1155253132716854\n",
      "239 1.0669577445290537\n",
      "240 1.0205396542411815\n",
      "241 0.9761533204798627\n",
      "242 0.9337264070184113\n",
      "243 0.893163187974757\n",
      "244 0.8543751323414484\n",
      "245 0.8172961065900546\n",
      "246 0.7818511431559925\n",
      "247 0.7479496621441641\n",
      "248 0.7155286576990586\n",
      "249 0.6845371947752684\n",
      "250 0.6548961520698954\n",
      "251 0.6265574836724324\n",
      "252 0.5994567384223538\n",
      "253 0.5735418325810495\n",
      "254 0.5487518470122683\n",
      "255 0.5250446618411\n",
      "256 0.5023741461965836\n",
      "257 0.48069005388061564\n",
      "258 0.459952929140897\n",
      "259 0.4401210102551836\n",
      "260 0.4211545693636639\n",
      "261 0.4030068262982155\n",
      "262 0.3856472550077501\n",
      "263 0.3690416105433765\n",
      "264 0.35316444388765994\n",
      "265 0.3379723450743616\n",
      "266 0.323437337035313\n",
      "267 0.30953555675613753\n",
      "268 0.2962391778138716\n",
      "269 0.28351435633000033\n",
      "270 0.2713443444093726\n",
      "271 0.2597011746324709\n",
      "272 0.248559748330094\n",
      "273 0.2378998720979691\n",
      "274 0.22770084805577423\n",
      "275 0.21794396823901185\n",
      "276 0.20861051549762033\n",
      "277 0.19967845673315493\n",
      "278 0.19112941140867865\n",
      "279 0.18295229129047796\n",
      "280 0.17512569650663143\n",
      "281 0.16763624998884047\n",
      "282 0.16047057273367776\n",
      "283 0.1536146778292305\n",
      "284 0.1470525976532776\n",
      "285 0.14077384204423365\n",
      "286 0.13476353232405686\n",
      "287 0.12901339273524262\n",
      "288 0.12351054930274617\n",
      "289 0.1182424606976322\n",
      "290 0.11320061715869638\n",
      "291 0.10837667663289832\n",
      "292 0.10375923535744674\n",
      "293 0.09934028786106938\n",
      "294 0.09511149141482853\n",
      "295 0.09106350509614633\n",
      "296 0.087187981417547\n",
      "297 0.08347831937211575\n",
      "298 0.07992759238857075\n",
      "299 0.07653029637630576\n",
      "300 0.07327736455057923\n",
      "301 0.07016315918404034\n",
      "302 0.06718244327947033\n",
      "303 0.06433028419839851\n",
      "304 0.061598879223500026\n",
      "305 0.058984680205400736\n",
      "306 0.05648148949834497\n",
      "307 0.05408592896856056\n",
      "308 0.0517919899646027\n",
      "309 0.04959583772934073\n",
      "310 0.04749383820704351\n",
      "311 0.04548181080123994\n",
      "312 0.04355529739511638\n",
      "313 0.04171029060853522\n",
      "314 0.03994392791084633\n",
      "315 0.03825310995561544\n",
      "316 0.036634699671607235\n",
      "317 0.03508436654697352\n",
      "318 0.03360005809202134\n",
      "319 0.03217908161075578\n",
      "320 0.030818774882951254\n",
      "321 0.029516262904726004\n",
      "322 0.028268831501640834\n",
      "323 0.027074387402613365\n",
      "324 0.02593083192512773\n",
      "325 0.024835582013678623\n",
      "326 0.023786883015068462\n",
      "327 0.022783066213720105\n",
      "328 0.021821736849691277\n",
      "329 0.020901021080715752\n",
      "330 0.020019390216414804\n",
      "331 0.019174965351316457\n",
      "332 0.01836665333406602\n",
      "333 0.017592369746994158\n",
      "334 0.016850764996056637\n",
      "335 0.016140658881637477\n",
      "336 0.015460610792891426\n",
      "337 0.0148095181357629\n",
      "338 0.01418594854778979\n",
      "339 0.01358868842574602\n",
      "340 0.013016562908828003\n",
      "341 0.01246881917989081\n",
      "342 0.011944097934879192\n",
      "343 0.011441622335660089\n",
      "344 0.010960353133380778\n",
      "345 0.01049945820818848\n",
      "346 0.010057951899401521\n",
      "347 0.009635153432795546\n",
      "348 0.009230225453829448\n",
      "349 0.008842328798619264\n",
      "350 0.008470841393008167\n",
      "351 0.008114930130391208\n",
      "352 0.007774077977554134\n",
      "353 0.00744761973615179\n",
      "354 0.00713498178751893\n",
      "355 0.0068354208708261\n",
      "356 0.0065485921024275055\n",
      "357 0.006273722981411687\n",
      "358 0.006010525835401911\n",
      "359 0.005758405561883914\n",
      "360 0.005516889118904523\n",
      "361 0.005285515143250822\n",
      "362 0.005063862670569831\n",
      "363 0.004851620691007773\n",
      "364 0.004648266460026173\n",
      "365 0.004453474081072027\n",
      "366 0.004266823017974184\n",
      "367 0.0040880466898433375\n",
      "368 0.003916818137377597\n",
      "369 0.003752773067714034\n",
      "370 0.003595582827468295\n",
      "371 0.0034450098880049987\n",
      "372 0.0033007821905536297\n",
      "373 0.0031626409880109773\n",
      "374 0.0030303008793589044\n",
      "375 0.0029034699205776203\n",
      "376 0.002781970274665647\n",
      "377 0.0026656242648143945\n",
      "378 0.002554114945158402\n",
      "379 0.0024472852895973613\n",
      "380 0.0023449308400703573\n",
      "381 0.002246871740191062\n",
      "382 0.0021529762359671465\n",
      "383 0.002062978276989846\n",
      "384 0.001976754300771733\n",
      "385 0.0018941287656997602\n",
      "386 0.0018149841421140603\n",
      "387 0.0017391519211005678\n",
      "388 0.0016664911886810354\n",
      "389 0.001596886285450441\n",
      "390 0.0015301794194178273\n",
      "391 0.0014663058714461906\n",
      "392 0.0014050744403311605\n",
      "393 0.001346416279454928\n",
      "394 0.0012902160522869125\n",
      "395 0.0012363616635486798\n",
      "396 0.0011847707358569235\n",
      "397 0.0011353215796369497\n",
      "398 0.0010879476246349875\n",
      "399 0.001042557303020207\n",
      "400 0.0009990745525287194\n",
      "401 0.000957407916453217\n",
      "402 0.0009174730434536001\n",
      "403 0.0008792121354657041\n",
      "404 0.0008425521199399218\n",
      "405 0.0008074217840293574\n",
      "406 0.0007737637367384578\n",
      "407 0.0007415074574230022\n",
      "408 0.000710603487685158\n",
      "409 0.0006809958543617536\n",
      "410 0.000652618799905907\n",
      "411 0.0006254286489212288\n",
      "412 0.000599367111102603\n",
      "413 0.0005743995334607266\n",
      "414 0.0005504712566915678\n",
      "415 0.0005275447805866489\n",
      "416 0.0005055770185752961\n",
      "417 0.00048452420959985896\n",
      "418 0.0004643523267484619\n",
      "419 0.00044501693201264703\n",
      "420 0.0004264902265393539\n",
      "421 0.00040873807865204637\n",
      "422 0.0003917228143649495\n",
      "423 0.00037542050725792534\n",
      "424 0.00035979445447399027\n",
      "425 0.000344826876095837\n",
      "426 0.0003304858526360288\n",
      "427 0.00031673546795410294\n",
      "428 0.0003035586973221461\n",
      "429 0.00029092943029017425\n",
      "430 0.0002788282865775887\n",
      "431 0.00026723305783381214\n",
      "432 0.00025611981365657904\n",
      "433 0.0002454685095280145\n",
      "434 0.00023526187370709225\n",
      "435 0.0002254820469395358\n",
      "436 0.00021610975707107052\n",
      "437 0.00020712671675944937\n",
      "438 0.00019851585162588433\n",
      "439 0.00019026403162343452\n",
      "440 0.00018235527548260476\n",
      "441 0.00017477993704616079\n",
      "442 0.00016751836408296578\n",
      "443 0.00016055840938698573\n",
      "444 0.00015388722627640483\n",
      "445 0.00014749290721519722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446 0.0001413673501174279\n",
      "447 0.00013549607606946212\n",
      "448 0.00012986781105016456\n",
      "449 0.00012447331010598473\n",
      "450 0.00011930318045401221\n",
      "451 0.00011435031613416877\n",
      "452 0.00010960341688455941\n",
      "453 0.00010505223428833738\n",
      "454 0.00010069033398567732\n",
      "455 9.650978917103353e-05\n",
      "456 9.250411363368732e-05\n",
      "457 8.866542286947446e-05\n",
      "458 8.498491701555878e-05\n",
      "459 8.145719125880266e-05\n",
      "460 7.807695759714833e-05\n",
      "461 7.483724849898668e-05\n",
      "462 7.17324287126611e-05\n",
      "463 6.875566234084958e-05\n",
      "464 6.590251200148065e-05\n",
      "465 6.316842233960662e-05\n",
      "466 6.054758487804845e-05\n",
      "467 5.8036167488538004e-05\n",
      "468 5.5628758511958806e-05\n",
      "469 5.33219413822327e-05\n",
      "470 5.11106960015646e-05\n",
      "471 4.899059491287657e-05\n",
      "472 4.695909268694033e-05\n",
      "473 4.501231119337365e-05\n",
      "474 4.3145803162290264e-05\n",
      "475 4.135689069814485e-05\n",
      "476 3.96418809152735e-05\n",
      "477 3.7998421061070184e-05\n",
      "478 3.642363256061759e-05\n",
      "479 3.4913779291463726e-05\n",
      "480 3.346643879315315e-05\n",
      "481 3.207904278974669e-05\n",
      "482 3.074931246598656e-05\n",
      "483 2.9474923786801746e-05\n",
      "484 2.8253676947391997e-05\n",
      "485 2.7082655999678477e-05\n",
      "486 2.59605163007172e-05\n",
      "487 2.4884710083073054e-05\n",
      "488 2.3853706933221082e-05\n",
      "489 2.286577589561094e-05\n",
      "490 2.1918292445221857e-05\n",
      "491 2.1010234394055188e-05\n",
      "492 2.0139837665653372e-05\n",
      "493 1.9305579235756726e-05\n",
      "494 1.850605018392607e-05\n",
      "495 1.7739685460622125e-05\n",
      "496 1.700485184550822e-05\n",
      "497 1.630052453006398e-05\n",
      "498 1.5625415378674308e-05\n",
      "499 1.4978259461699794e-05\n"
     ]
    }
   ],
   "source": [
    "# 用Numpy 来写一个两层的ANN\n",
    "import numpy as np\n",
    "# Batch_size, input_dimension, hidden_dimension, output_dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# random input and target from uniform distribution\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# random weights from uniform distribution\n",
    "# 这里hidden vector 不是初始化出来的，而是通过input 和 w1 相乘计算出来的\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "#     Dot product of two arrays. \n",
    "# If both a and b are 2-D arrays, it is matrix multiplication, but using matmul or a @ b is preferred.\n",
    "    h = x.dot(w1)\n",
    "#     这里会返回elementwise maximum array，但是注意这里与torch.max不一样，torch.max是求出一个array按行或列返回最大值indices\n",
    "#     numpy中对应的函数是np.amax()\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "#     计算square loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "#     直接手动写出loss function对y_pred以及前面weight matrix的gradient，而PyTorch中基本能对任意可导函数微分\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/chain_rule_differentiation.png\" alt=\"图片替换文本\" width=\"600\" height=\"300\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 943.206787109375\n",
      "199 3.445002555847168\n",
      "299 0.019996928051114082\n",
      "399 0.0002746207173913717\n",
      "499 4.034971789224073e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "#     forward pass\n",
    "#   torch.mm(matrix1, matrix2) matrix mulitpliction\n",
    "    h = x.mm(w1)\n",
    "#   torch.clamp(input, min, max) 能将tensor限制在[min, max]范围内，假如是NN的话，请用F.relu\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h.mm(w2)\n",
    "    \n",
    "#   这里没有像np或TF1一样使用square function\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "        \n",
    "#     backward pass\n",
    "    grad_y_pred = 2 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "#   Ndarray.copy() 被Tensor.clone() 所代替\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 686.5114135742188\n",
      "199 3.257018566131592\n",
      "299 0.019082289189100266\n",
      "399 0.0002951398491859436\n",
      "499 4.0182305383495986e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Tensors are nodes in the computation graph, edges are functions generating the values in the tensors.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "#     因为有autograd，我们不需要手动把intermediate values保存下来，可以一步直接计算到output\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "#    这里不能用Tensor.item()，因为这里的loss 一开始是一个Tensor，Tensor才能用来backward\n",
    "#    如果用了.item()，直接返回一个Python scalar\n",
    "#    这里与前面的例子不同的地方在于前面的例子loss只是一个representation，没有实际的意义\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "#        在backward以后，所有Tensor.grad will hold the gradient of the loss with respect to w1 and w2\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#       这一部是手动更新weight，也可以用optim.SGD，计算这部分的时候和inference一样都是No gradient的\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "#         update weights以后手动清空gradients\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining new autograd functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 645.6539916992188\n",
      "199 4.855976581573486\n",
      "299 0.0506538562476635\n",
      "399 0.0008095258381217718\n",
      "499 8.064862049650401e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 其实 autograd 就是一个forward，一个backward，所以我们可以实现forward和backward的方法，\n",
    "# 然后subclass torch.autograd.Function，最后像个function一样召唤它\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "#         这里的ctx是一个context object，只是用来stash information for backward computation\n",
    "#         可以用ctx.save_for_backward这种方法\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "#        元组解包\n",
    "        input, = ctx.saved_tensors\n",
    "#     这里我们已经接收到Tensor的grad，所以可以计算考虑input的gradient\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input<0] = 0\n",
    "        return grad_input\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Tensors are nodes in the computation graph, edges are functions generating the values in the tensors.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# 给MyReLU 起一个别名，apply以后会自动调用MyReLU的forward和backward method \n",
    "relu = MyReLU.apply\n",
    "\n",
    "for t in range(500):\n",
    "#     除了这一步，别的步骤跟前面别无二致\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1.5782206058502197\n",
      "199 0.01805734634399414\n",
      "299 0.000392022862797603\n",
      "399 1.0980829756590538e-05\n",
      "499 3.4718257779786654e-07\n"
     ]
    }
   ],
   "source": [
    "# High level abstractions over raw computational graphs useful for building neural network\n",
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out))\n",
    "\n",
    "# 创建一个MSEloss的instance\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "#     \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t% 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 41.959007263183594\n",
      "199 0.48242470622062683\n",
      "299 0.0015302618267014623\n",
      "399 4.968972461938392e-06\n",
      "499 2.4410873322722182e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out))\n",
    "\n",
    "# 创建一个MSEloss的instance\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# 第一个参数告诉optim要优化哪些参数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "#     \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t% 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Custom NN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 54.945655822753906\n",
      "199 0.9118472337722778\n",
      "299 0.004170420579612255\n",
      "399 4.6487789404636715e-06\n",
      "499 1.4031473760667268e-09\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "net = Net(D_in, H, D_out)\n",
    "\n",
    "# 创建一个MSEloss的instance\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# 第一个参数告诉optim要优化哪些参数\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = net(x)\n",
    "#     \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t% 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Control flow and Weight sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 651.8897094726562\n",
      "199 268.1856994628906\n",
      "299 149.90826416015625\n",
      "399 26.771236419677734\n",
      "499 13.140218734741211\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, H)\n",
    "        self.linear3 = torch.nn.Linear(H, D_out)\n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.linear2(h_relu).clamp(min=0)\n",
    "        y_pred = self.linear3(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "net = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 创建一个MSEloss的instance\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# 第一个参数告诉optim要优化哪些参数\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    y_pred = net(x)\n",
    "#     \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t% 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
